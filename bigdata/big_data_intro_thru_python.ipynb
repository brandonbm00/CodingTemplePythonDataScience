{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to PySpark and Amazon Elastic Map-Reduce (EMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PySpark Specific Machinery\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SparkSession object represents your entrypoint to the cluster. It is the single most important object in Spark and serves as the mechanism through which you will interact with the Hadoop resources provisioned for you. Every Spark \"job\" - i.e. an individual spark applicaton run by a user, will have a uniqe SparkContext that in some abstract sense 'contains' the resources allocated to that job at all times. For that reason each job only has one SparkContext - if you try and create a second one, you'll get an error (depending on cluster configuration parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"yarn\")\\\n",
    "    .config(\"spark.driver.cores\", 1)\\\n",
    "    .appName(\"demo_spark\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-32-61.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>demo_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb7751da748>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can exit the SparkSession using the following switch, however if you need the resources back you might need to wait for your cluster availability to swing back in your favor (so be careful especially if you suspect your co-workers overuse cluster resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark.stop() # shuts down the sparkcontext - but not the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs are the core object of study in Spark and form the backbone of the architecture that makes it work. An RDD can be thought of as a complex plan for creating a dataset in parallel - a dataset that will in the end, exist on disk in many pieces. Since it starts in many pieces, the RDD represents the set of partition level plans to build the dataset from scratch in parallel. When thought of in this way it makes sense that if something happens and a single node fails, Spark doesn't have to regenerate the entire dataset - it can just regenerate the specific partitions related to the node that failed. This is very in-pattern when considered as an application of the Hadoop infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Figure needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(13) MapPartitionsRDD[13] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |   MapPartitionsRDD[12] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |   MapPartitionsRDD[11] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |   FileScanRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0 []'\n"
     ]
    }
   ],
   "source": [
    "print(data.rdd.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Tuning Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel I/O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Dataset Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAGs and Lazy Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy execution is a feature built into spark designed both to make development easier for you as well as minimize overuse of cluster resources. In one sentence, it means that Spark doesn't compute anything it doesn't absolutely have to. In Spark, you write the majority of the program in a manner that specifies a PLAN of WHAT youw WANT to happen - and then trigger the actual execution of the plan later. Let's see how this actually works in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.read.parquet('data/subset_reviews_parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [Id#0,ProductId#1,UserId#2,ProfileName#3,HelpfulnessNumerator#4,HelpfulnessDenominator#5,Score#6,Time#7,Summary#8,Text#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ip-172-31-32-61.ec2.internal:8020/user/hadoop/data/subset_reviews_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Id:string,ProductId:string,UserId:string,ProfileName:string,HelpfulnessNumerator:string,He...\n"
     ]
    }
   ],
   "source": [
    "data.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_scores = data.where(F.col('Score') == '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: string, ProductId: string, UserId: string, ProfileName: string, HelpfulnessNumerator: string, HelpfulnessDenominator: string, Score: string, Time: string, Summary: string, Text: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [Id#0, ProductId#1, UserId#2, ProfileName#3, HelpfulnessNumerator#4, HelpfulnessDenominator#5, Score#6, Time#7, Summary#8, Text#9]\n",
      "+- *(1) Filter (isnotnull(Score#6) && (Score#6 = 5))\n",
      "   +- *(1) FileScan parquet [Id#0,ProductId#1,UserId#2,ProfileName#3,HelpfulnessNumerator#4,HelpfulnessDenominator#5,Score#6,Time#7,Summary#8,Text#9] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ip-172-31-32-61.ec2.internal:8020/user/hadoop/data/subset_reviews_parquet], PartitionFilters: [], PushedFilters: [IsNotNull(Score), EqualTo(Score,5)], ReadSchema: struct<Id:string,ProductId:string,UserId:string,ProfileName:string,HelpfulnessNumerator:string,He...\n"
     ]
    }
   ],
   "source": [
    "best_scores.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|Score|                Text|\n",
      "+-----+--------------------+\n",
      "|    5|These chips were ...|\n",
      "|    5|these are the bes...|\n",
      "|    5|\"At first I was t...|\n",
      "|    5|I try to be good,...|\n",
      "|    5|We've long enjoye...|\n",
      "|    5|\"Kettle lightly s...|\n",
      "|    5|Fantastic tasting...|\n",
      "|    5|Great for HS lunc...|\n",
      "|    5|My 8 yo is a Kett...|\n",
      "|    5|This may sound re...|\n",
      "|    5|I bought my first...|\n",
      "|    5|A Ming Dynasty (A...|\n",
      "|    5|I enjoy a number ...|\n",
      "|    5|I have an okra lo...|\n",
      "|    5|\"My just turned 4...|\n",
      "|    5|\"My just turned 4...|\n",
      "|    5|My 18 month old l...|\n",
      "|    5|My kids love thes...|\n",
      "|    5|These organic pou...|\n",
      "|    5|We absolutely lov...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_scores.select('Score', 'Text').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long story short, doing something to a datset via some code in this notebook doesn't actually do anything to materialize the real data until you force it too - you can think of successive dataframe manipulations as simply adding steps to the plan for what will later happen. This is advantageous for the programmer as well as the cluster - you don't want to have to compute the entire dataset every time you want to change something. You can make it pretty far without actually materializing anything however - generally when developing data science workflows you make many errors just trying to match up the right metadata anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv('data/Subset_reviews.csv', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
      "| Id| ProductId|        UserId|         ProfileName|HelpfulnessNumerator|HelpfulnessDenominator|Score|      Time|             Summary|                Text|\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
      "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|\n",
      "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|\n",
      "|  3|B000LQOCH0| ABXLMWJIXXAIN|\"Natalia Corres \"...|                   1|                     1|    4|1219017600|\"\"\"Delight\"\" says...|\"This is a confec...|\n",
      "|  4|B000UA0QIQ|A395BORC6FGVXV|                Karl|                   3|                     3|    2|1307923200|      Cough Medicine|If you are lookin...|\n",
      "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|\n",
      "|  6|B006K2ZZ7K| ADT0SRK1MGOEU|      Twoapennything|                   0|                     0|    4|1342051200|          Nice Taffy|I got a wild hair...|\n",
      "|  7|B006K2ZZ7K|A1SP2KVKFXXRU1|   David C. Sullivan|                   0|                     0|    5|1340150400|Great!  Just as g...|This saltwater ta...|\n",
      "|  8|B006K2ZZ7K|A3JRGQVEQN31IQ|  Pamela G. Williams|                   0|                     0|    5|1336003200|Wonderful, tasty ...|This taffy is so ...|\n",
      "|  9|B000E7L2R4|A1MZYO9TZK0BBI|            R. James|                   1|                     1|    5|1322006400|          Yay Barley|Right now I'm mos...|\n",
      "| 10|B00171APVA|A21BT40VZCCYT4|       Carol A. Reed|                   0|                     0|    5|1351209600|    Healthy Dog Food|This is a very he...|\n",
      "| 11|B0001PB9FE|A3HDKO7OW0QNK4|        Canadian Fan|                   1|                     1|    5|1107820800|The Best Hot Sauc...|I don't know if i...|\n",
      "| 12|B0009XLVG0|A2725IB4YY9JEB|\"A Poeng \"\"Sparky...|                   4|                     4|    5|1282867200|\"My cats LOVE thi...|One of my boys ne...|\n",
      "| 13|B0009XLVG0| A327PCT23YH90|                  LT|                   1|                     1|    1|1339545600|My Cats Are Not F...|My cats have been...|\n",
      "| 14|B001GVISJM|A18ECVX2RJ7HUE| \"willie \"\"roadie\"\"\"|                   2|                     2|    4|1288915200|   fresh and greasy!|good flavor! thes...|\n",
      "| 15|B001GVISJM|A2MUGFV2TDQ47K|\"Lynrie \"\"Oh HELL...|                   4|                     5|    5|1268352000|Strawberry Twizzl...|The Strawberry Tw...|\n",
      "| 16|B001GVISJM|A1CZX3CP8IKQIJ|        Brian A. Lee|                   4|                     5|    5|1262044800|Lots of twizzlers...|My daughter loves...|\n",
      "| 17|B001GVISJM|A3KLWF6WQ5BNYO|      Erica Neathery|                   0|                     0|    2|1348099200|          poor taste|I love eating the...|\n",
      "| 18|B001GVISJM| AFKW14U97Z6QO|               Becca|                   0|                     0|    5|1345075200|            Love it!|I am very satisfi...|\n",
      "| 19|B001GVISJM|A2A9X58G2GTBLP|             Wolfee1|                   0|                     0|    5|1324598400|  GREAT SWEET CANDY!|Twizzlers, Strawb...|\n",
      "| 20|B001GVISJM|A3IV7CL2C13K2U|                Greg|                   0|                     0|    5|1318032000|Home delivered tw...|Candy was deliver...|\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- ProductId: string (nullable = true)\n",
      " |-- UserId: string (nullable = true)\n",
      " |-- ProfileName: string (nullable = true)\n",
      " |-- HelpfulnessNumerator: string (nullable = true)\n",
      " |-- HelpfulnessDenominator: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.write.parquet('data/subset_reviews_parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         Score|count|\n",
      "+--------------+-----+\n",
      "|       Hugs\"\"\"|    1|\n",
      "|             7|    7|\n",
      "|     friend\"\"\"|    1|\n",
      "|            11|    1|\n",
      "|             3| 8019|\n",
      "|             8|    1|\n",
      "| and Kitten\"\"\"|   26|\n",
      "|        Dad\"\"\"|    1|\n",
      "|            16|    1|\n",
      "|             0|  206|\n",
      "|            47|    5|\n",
      "|   Comp sci\"\"\"|    1|\n",
      "|          null|    2|\n",
      "|             5|62148|\n",
      "|            18|    1|\n",
      "|       blo...\"|    1|\n",
      "|     Medit...\"|    1|\n",
      "|            17|    1|\n",
      "|     Lyme ...\"|    1|\n",
      "|          ...\"|    7|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('Score').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.read.parquet('data/subset_reviews_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- ProductId: string (nullable = true)\n",
      " |-- UserId: string (nullable = true)\n",
      " |-- ProfileName: string (nullable = true)\n",
      " |-- HelpfulnessNumerator: string (nullable = true)\n",
      " |-- HelpfulnessDenominator: string (nullable = true)\n",
      " |-- Score: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [Id#143,ProductId#144,UserId#145,ProfileName#146,HelpfulnessNumerator#147,HelpfulnessDenominator#148,Score#149,Time#150,Summary#151,Text#152] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://ip-172-31-32-61.ec2.internal:8020/user/hadoop/data/subset_reviews_parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Id:string,ProductId:string,UserId:string,ProfileName:string,HelpfulnessNumerator:string,He...\n"
     ]
    }
   ],
   "source": [
    "data.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|             Score|count|\n",
      "+------------------+-----+\n",
      "|                 5|62148|\n",
      "|                 4|14537|\n",
      "|                 1| 9392|\n",
      "|                 3| 8019|\n",
      "|                 2| 5582|\n",
      "|                 0|  206|\n",
      "|     and Kitten\"\"\"|   26|\n",
      "|   book-blogger\"\"\"|   13|\n",
      "|                 6|    9|\n",
      "|                 7|    7|\n",
      "|              ...\"|    7|\n",
      "|             RN\"\"\"|    6|\n",
      "|                47|    5|\n",
      "|                10|    5|\n",
      "|     Music Fan...\"|    3|\n",
      "|                14|    3|\n",
      "| and book lover\"\"\"|    3|\n",
      "|        swimme...\"|    2|\n",
      "|             a...\"|    2|\n",
      "|                33|    2|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('Score').count().sort(F.col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Pandas >= 0.19.2 must be installed; however, your version was 0.18.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2f800dec27fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1933\u001b[0m         \"\"\"\n\u001b[1;32m   1934\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_minimum_pandas_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1935\u001b[0;31m         \u001b[0mrequire_minimum_pandas_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimum_pandas_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         raise ImportError(\"Pandas >= %s must be installed; however, \"\n\u001b[0;32m--> 129\u001b[0;31m                           \"your version was %s.\" % (minimum_pandas_version, pandas.__version__))\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Pandas >= 0.19.2 must be installed; however, your version was 0.18.1."
     ]
    }
   ],
   "source": [
    "data_pd = data.groupBy('Score').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
